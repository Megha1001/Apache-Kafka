************KAFKA THEORY************

Kafka Use Cases
-----------------
- Messaging System
- Activity Tracking
- Gather metrics from differnt locations
- Application logs gathering
- Stream processing(with kafka Stream API for example)
- Integration with Spark, Hadoop, flink and many BIg Data techs
- Can be used as microservice pub/sub

REMEMBER THAT KAKFA IS ONLY USED AS A TRANSPORTATION MECHANISM

Kafka Topics
-----------------

- A particular stream of data
- Like a table in a database(without all the contraints)
- You can have as many topics as you want
- A topic is identified by its name
- Supports any type of message format(JSON, AVRO, TXT etc)
- The sequence of messages is called a data stream.
- You cannot query topics, instead , use kafka producers to send data and kafka consumers to read the data


Partitions and offsets
------------------------

- Topics are split in partitions(example, 100 partitions)
- Messages send to topics are endup in these partitions
    - Messages within each partition are ordered
    - Each message within a partition gets an incremental id, called offset
- kafka topics are immutable : once data is written to a partition , it cannot be changed

- Once the data is written to a partition, it cannot be changed(immutability)
- Data is kept only for a limited time(configurable)
- Offset only have a meaning for a specific partition
    - E.g. offset 3 in partition 0 doesn't represent the same data as offset 3 in partition 1
    - Offset are not re-used even if previous message have been deleted
- Order is guaranteed only within a partition(not across partitions)
- data is assigned randomly to a partition unless a key is provided
- You can have as many partitions per topic as you want.




PRODUCERS
-----------------

- Producers write data to topics(which are made of partitions)
- Producers know to which partition to write to( and which kafka broker has it). Producer decided in which partition data to write to not kafka servers.
- In case of kafka broker failures, Producers will automatically recover.
    - The load is balanced to many brokers thanks to the number of partitions


Producers : Message keys
- Producers can choose to send a key with the message(Optional) [string, number, binary]
- if key=null, data is sent round robin(partition 0, then 1, then 2)
- If key!=null, then all messages for that key will always go to the same partition(hashing)
- A key are typically sent if you need messages ordering for a specific field(ex : truck_id) [Two keys can endup in the same partition after hashing]


Kafka Message Anantomy
-------------------------
- See image attached

Kafka Message Serializer
---------------------------
- Kafka only accepts bytes as an input from producers and sends bytes out as an output to consumers.
- Message serialization means tranforming objects/data into bytes.
- They are used on the value and the key.
    - Ex :
        - Key Object (123) -> int -> keySerializer=IntergerSerializer -> bytes -> key-binary(01110011)
        - Value Object ("Hello World") -> String -> ValueSerializer=StringSerializer -> bytes -> Value-binary(01101110101010111100001101)
- Common Serializer
    - String(incl. JSON)
    - Int, Float
    - Avro
    - Protobuf


Kakfa Message Key Hashing
---------------------------
- A kafka partitioner is a code logic that takes a record and determines to which partitin to sent it into.
Record -> .send() -> Producer Partitioner  logic -> Assign partition1 -> Partition1

- Key hashing is the process of determining the mappint of a key to a partition.
- In the default kafka partitioner, the keys are hashed using the murmur2 algorithm, with the formula
    targetPartition = Math.abs(Utils.murmur2(keyBytes))%(numPartitions - 1)

THIS IS JUST TO STRESS THE FACT THAT PRODUCERS ARE THE ONE WHO CHOOSE IN WHICH PARTITION MESSAGE IS SENT TO



CONSUMERS
----------
- Consumers read data from the topic(identified by name) - pull model
- Consumers automatically know which borker to read from
- In case of broker failures, consumers know how to recover
- Data is read in order from low to high offset within each partitions


Consumer Deserializer
-----------------------
- Deserialize indicates how to transform bytes into objects/data
- Consumers should know in advance in which format your data is
- They are used on the value and key of the message(since we serialized them only when sending to kafka in producer)
    - Ex :
        - key-binary(01110011) -> bytes -> keyDeserializer=IntergerDeserializer -> int -> Key Object (123)
        - Value-binary(01101110101010111100001101) ->bytes -> ValueDeserializer=StringDeserializer ->   String-> Value Object ("Hello World")
- Common Deserializer
    - String(incl. JSON)
    - Int, Float
    - Avro
    - Protobuf

Note : The serialization/deserialization type must not change during the topic lifecycle(create a new topic instead)


Consumer Groups
-------------------
- All the consumers in an application read data as a consumer groups.
- Each consumer within a group reads from exclusive partitions.

What if too many consumers ?
- If you have more consumers than partitions, some consumers will be inactive.
- In apache kafka its acceptable to have multiple consumer group on the same topic
- To create distinct consumer groups, use the consumer property group.id


Consumer Offsets
--------------------
- kafka stores the offset at which a consumer group has been reading
- The offsets committed kafka topic named __consumer_offsets
- When a consumer in a group has processed data received from kafka, it should be periodically committig the offsets (the kafka broker will write to __consumer_offsets, not the group itself) --> that's why consumer within 
consumer group reads exclusively.
- If a consumer dies, it will be able to read back from where it left off thanks to the committed cosumer offsets!


Deliver semantics for consumers
----------------------------------
- By default, Java consumers will automaticaly commit offsets(at least once)
- There are 3 delivery semantics if you choose to commit manually
- At least once(Usually preferred)
    - Offsets are committed after the mesage is processed
    - If the procesing goes wrong, the message will be read again
    - This can result in duplicate processing of message, Make sure your processing is idempotent(i.e. processing again the messages won't impact your systems)
- At most once
    - Offsets are commited as soon as messages are received
    - If the processing goes wrong, some messages will be lost(they won't rbe read again)
- Exactly once
    - For kafka => kafka workflows : use the transactional API(easy with Kafka Streams API)
    - For Kafka => Exteral System workflows : use an idempotent consumer


KAFKA BROKERS
-------------
- A kafka cluster is composed of multiple brokers(servers)
- Each broker is identified with its ID(Integer)
- Each broker contains certain topic partitions(meaning your data is going to be distributes across all brokers)
- After connecting to any broker(called a boostrap broker), you will be connected to the entire cluster(kafka clients have smart mechanics for that)


kafka Broker Discovery
--------------------------
- Every kafka broker is also called a "bootstrap server"
- That mean that you only need to connect to one broker, and the kafka clients will know how to be connected to the entire cluster(smart clients)
- Each broker knows about all brokers, topics and partitions(metadata)



Topic replication factor
--------------------------
- Topics should have a replication factor >1 (usually between 2 and 3) (2 : total 2 entities will be there(1original + 1 copy))
- This way if a broker is down, another broker can serve the data

Consumer Leader for a partition
----------------------------
- At any time only ONE broker can be a leader for a given partition
- Producers can only send data to the broker that is leader of a partition
- The other brokers will replicate the data
- Therefore, each partition has one leader and multiple ISR(in-sync replica)


Default Producer and consumer behavior with leaders
---------------------------------------------------
- Kafka producers can only write to the leader broker for a partition
- Kafka consumers by default will read form the leader broker for a partition


Kafka consumers Replica Fetching (Kafka v2.4+)
-----------------------------------------------
- Since Kafka v2.4, its possible to configure consumers to read form the closest replica
- This may help improve latency, and also decrease netwrok costs if using the cloud

Producer -----> Broker101(Topic-A Partition)(Leader)) --> replication --> Broker102(Topic-A(Partition0(ISR))) --> consumer can read from Broker102



Producer Acknowledgements(acks)
--------------------------------
- PRocducers can choose to receive acknowledgment of data writes:
    - acks=0 : Producer won't wait for acknowledgemnt(possible data loss)
    - acks=1 : Producer will wait for leader acknowledgement(limited data loss)
    - acks=2 : Producder will wait for leader + replicas acknowledgment(no data loss)


Kakfa Topic durability
----------------------
- For a topic replication factor of 3, topic data durability can withstand 2 brokers loss
- As a rule, for a replication factor of N, you can permanently lose up  to N-1 brokers and still recover your data



ZOOKEEPER
----------
- Zookeeper managers brokers(keeps a list of them)
- Its a software, its like a companion to brokers
- Zookeeper helps in performing leader election for partitions(If brokers goes down zookeeper helps to elect new leader)
- Zookeeeper sends notifiction to Kafka/ kafka broker in case of changes
(e.g. new topic, broker dies, borker comes up, delete topics etc)

KAFKA 2.X CAN'T WORK WITHOUT ZOOKEEPER
KAFKA 3.X CAN WORK WITHOUT ZOOKEEPER(KIP-500)- USING KAKFKA RAFT(AKA KRAFT) INSTEAD
KAFKA 4.X WILL NOT HAVE ZOOKEEPER

- Zookeeper by design  operates with an odd number of servers(1,3,5,7). Here 1, 3, 5, 7 are number of zookeepers
- Zookeeper has a leader(writes) the rest of the server are followers(reads)
- Zookeeper does NOT store consumer offsets with kafka>0v0.10 (as we store it in interal __consumer_offset topic now)


Should you use Zookeeper ?
-------------------------
With kafka Brokers ?
 - if we are managing kafka brokers the Yes, until kafka 4.0 is out while waiting for kafka without zookeeper to be production ready


With Kafka Clients
    - Earlier producer, consumers brokers used to connect with zookeeper but not anymore they have been migrated

    - OVertime, the kafka clients and CLI have been migrated to leverage the brokers as a connection endpoint instead of ZOokeeper(basically kafka clients and CLI can connect to kafka brokers directly)
    - Since kafka 0.10, consumers store offset in kafka  and must not connect to zookeeper as its deprecated(earlier offset stores in zookeeper but not anymore)
    - Since kafka 2.2, the kafka-topics.sh CLI command reference kafka brokers not zookeepr for topic management(creation, deletion, etc....) and the Zookeeper CLI argument is deprecated.
    - All the APIs and commands that were previously leveraging Zookeeper are migrated to use kafka instead , so that when cluster are migrated to be without zookeeper , the change is invisible to clients.
    - Zookeeper less secure than kafka, and therefore zookeeper ports should only be opened to allow traffic from kafka brokers, and not kafka clients
    - Therefore, to be a great modern-day kafka developer, never ever use zookeeer as a configuration in your kafka-clients, and other programs that connect to kafka.




KAFKA KRAFT
------------
- In 2020, the apache kafka project started to work to remove zookeeper dependency from it(KIP-500)
- Zookeeper shows scaling issues when kafka clusters have > 100,000 partitions
- By removing zookeeper , apache kafka can
    - scale to millions of partitions, and becomes easier to maintain and setup
    - improve stability, makes it easier to monitor, support and administer
    - Single securtiy model for the whole system
    - single process to start with kafka
    - Faster controller shutdown and recovery time

- Kafka 3.X now implements the Raft protocol(KRaft) in order to replace zookeeper.
    - Production ready since Kafka 3.3.1(KIP-833)
    - Kakfa 4.0 will be released only with KRaft(no Zookeeper)



KAFKA INSTALLATION
https://learn.conduktor.io/kafka/how-to-install-apache-kafka-on-mac/



ONE KAFKA broker with ZooKeeper
---------------------------------
1. Start Zookeeper using the binaries
2. Start kafka using the binaries in another process

Kafka Cluster
    Broker101       ---> Zookeeper