Kafka Use Cases
-----------------
- Messaging System
- Activity Tracking
- Gather metrics from differnt locations
- Application logs gathering
- Stream processing(with kafka Stream API for example)
- Integration with Spark, Hadoop, flink and many BIg Data techs
- Can be used as microservice pub/sub

REMEMBER THAT KAKFA IS ONLY USED AS A TRANSPORTATION MECHANISM

Kafka Topics
-----------------

- A particular stream of data
- Like a table in a database(without all the contraints)
- You can have as many topics as you want
- A topic is identified by its name
- Supports any type of message format(JSON, AVRO, TXT etc)
- The sequence of messages is called a data stream.
- You cannot query topics, instead , use kafka producers to send data and kafka consumers to read the data


Partitions and offsets
------------------------

- Topics are split in partitions(example, 100 partitions)
- Messages send to topics are endup in these partitions
    - Messages within each partition are ordered
    - Each message within a partition gets an incremental id, called offset
- kafka topics are immutable : once data is written to a partition , it cannot be changed

- Once the data is written to a partition, it cannot be changed(immutability)
- Data is kept only for a limited time(configurable)
- Offset only have a meaning for a specific partition
    - E.g. offset 3 in partition 0 doesn't represent the same data as offset 3 in partition 1
    - Offset are not re-used even if previous message have been deleted
- Order is guaranteed only within a partition(not across partitions)
- data is assigned randomly to a partition unless a key is provided
- You can have as many partitions per topic as you want.




PRODUCERS
-----------------

- Producers write data to topics(which are made of partitions)
- Producers know to which partition to write to( and which kafka broker has it). Producer decided in which partition data to write to not kafka servers.
- In case of kafka broker failures, Producers will automatically recover.
    - The load is balanced to many brokers thanks to the number of partitions


Producers : Message keys
- Producers can choose to send a key with the message(Optional) [string, number, binary]
- if key=null, data is sent round robin(partition 0, then 1, then 2)
- If key!=null, then all messages for that key will always go to the same partition(hashing)
- A key are typically sent if you need messages ordering for a specific field(ex : truck_id) [Two keys can endup in the same partition after hashing]


Kafka Message Anantomy
-------------------------
- See image attached

Kafka Message Serializer
---------------------------
- Kafka only accepts bytes as an input from producers and sends bytes out as an output to consumers.
- Message serialization means tranforming objects/data into bytes.
- They are used on the value and the key.
    - Ex :
        - Key Object (123) -> int -> keySerializer=IntergerSerializer -> bytes -> key-binary(01110011)
        - Value Object ("Hello World") -> String -> ValueSerializer=StringSerializer -> bytes -> Value-binary(01101110101010111100001101)
- Common Serializer
    - String(incl. JSON)
    - Int, Float
    - Avro
    - Protobuf


Kakfa Message Key Hashing
---------------------------
- A kafka partitioner is a code logic that takes a record and determines to which partitin to sent it into.
Record -> .send() -> Producer Partitioner  logic -> Assign partition1 -> Partition1

- Key hashing is the process of determining the mappint of a key to a partition.
- In the default kafka partitioner, the keys are hashed using the murmur2 algorithm, with the formula
    targetPartition = Math.abs(Utils.murmur2(keyBytes))%(numPartitions - 1)

THIS IS JUST TO STRESS THE FACT THAT PRODUCERS ARE THE ONE WHO CHOOSE IN WHICH PARTITION MESSAGE IS SENT TO



CONSUMERS
----------
- Consumers read data from the topic(identified by name) - pull model
- Consumers automatically know which borker to read from
- In case of broker failures, consumers know how to recover
- Data is read in order from low to high offset within each partitions


Consumer Deserializer
-----------------------
- Deserialize indicates how to transform bytes into objects/data
- Consumers should know in advance in which format your data is
- They are used on the value and key of the message(since we serialized them only when sending to kafka in producer)
    - Ex :
        - key-binary(01110011) -> bytes -> keyDeserializer=IntergerDeserializer -> int -> Key Object (123)
        - Value-binary(01101110101010111100001101) ->bytes -> ValueDeserializer=StringDeserializer ->   String-> Value Object ("Hello World")
- Common Deserializer
    - String(incl. JSON)
    - Int, Float
    - Avro
    - Protobuf

Note : The serialization/deserialization type must not change during the topic lifecycle(create a new topic instead)


Consumer Groups
-------------------
- All the consumers in an application read data as a consumer groups.
- Each consumer within a group reads from exclusive partitions.

What if too many consumers ?
- If you have more consumers than partitions, some consumers will be inactive.
- In apache kafka its acceptable to have multiple consumer group on the same topic
- To create distinct consumer groups, use the consumer property group.id


Consumer Offsets
--------------------
- kafka stores the offset at which a consumer group has been reading
- The offsets committed kafka topic named __consumer_offsets
- When a consumer in a group has processed data received from kafka, it should be periodically committig the offsets (the kafka broker will write to __consumer_offsets, not the group itself) --> that's why consumer within 
consumer group reads exclusively.
- If a consumer dies, it will be able to read back fro where it left off thanks to the committed cosumer offsets!


Deliver semantics for consumers
----------------------------------
- By default, Java consumers will automaticaly commit offsets(at least once)
- There are 3 delivery semantics if you choose to commit manually
- At least once(Usually preferred)
    - Offsets are committed after the mesage is processed
    - If the procesing goes wrong, the message will be read again
    - This can result in duplicate processing of message, Make sure your processing is idempotent(i.e. processing again the messages won't impact your systems)
- At most once
    - Offsets are commited as soon as messages are received
    - If the processing goes wrong, some messages will be lost(they won't rbe read again)
- Exactly once
    - For kafka => kafka workflows : use the transactional API(easy with Kafka Streams API)
    - For Kafka => Exteral System workflows : use an idempotent consumer


KAFKA BROKERS
-------------
- A kafka cluster is composed of multiple brokers(servers)
- Each broker is identified with its ID(Integer)
- Each broker contains certain topic partitions(meaning your data is going to be distributes across all brokers)
- After connecting to any broker(called a boostrap broker), you will be connected to the entire cluster(kafka clients have smart mechanics for that)


kafka Broker Discovery
--------------------------
- Every kafka broker is also called a "bootstrap server"
- That mean that you only need to connect to one broker, and the kafka clients will know how to be connected to the entire cluster(smart clients)
- Eacg broker knows about all brokers, topics and partitions(metadata)



Topic replication factor
--------------------------
- Topics should have a replication factor >1 (usually between 2 and 3) (2 : total 2 entities will be there(1original + 1 copy))
- This way if a broker is down, another broker can serve the data

Conf Leader for a partition
----------------------------
- At any time only ONE broker can be a leader for a given partition
- Producers can only send data to the broker that is leader of a partition
